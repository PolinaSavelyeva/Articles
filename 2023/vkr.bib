% !TeX spellcheck = ru_RU

@incollection{LinearAlgebra-basedGraphFramework,
author = {Yang, Carl and Bulu\c{c}, Ayd\i{}n and Owens, John D.},
title = {GraphBLAST: A High-Performance Linear Algebra-Based Graph Framework on the GPU},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/3466795},
doi = {10.1145/3466795},
abstract = {High-performance implementations of graph algorithms are challenging to implement on new parallel hardware such as GPUs because of three challenges: (1)&nbsp;the difficulty of coming up with graph building blocks, (2)&nbsp;load imbalance on parallel hardware, and (3)&nbsp;graph problems having low arithmetic intensity. To address some of these challenges, GraphBLAS is an innovative, on-going effort by the graph analytics community to propose building blocks based on sparse linear algebra, which allow graph algorithms to be expressed in a performant, succinct, composable, and portable manner. In this paper, we examine the performance challenges of a linear-algebra-based approach to building graph frameworks and describe new design principles for overcoming these bottlenecks. Among the new design principles is exploiting input sparsity, which allows users to write graph algorithms without specifying push and pull direction. Exploiting output sparsity allows users to tell the backend which values of the output in a single vectorized computation they do not want computed. Load-balancing is an important feature for balancing work amongst parallel workers. We describe the important load-balancing features for handling graphs with different characteristics. The design principles described in this paper have been implemented in “GraphBLAST”, the first high-performance linear algebra-based graph framework on NVIDIA GPUs that is open-source. The results show that on a single GPU, GraphBLAST has on average at least an order of magnitude speedup over previous GraphBLAS implementations SuiteSparse and GBTL, comparable performance to the fastest GPU hardwired primitives and shared-memory graph frameworks Ligra and Gunrock, and better performance than any other GPU graph framework, while offering a simpler and more concise programming model.},
journal = {ACM Trans. Math. Softw.},
month = {feb},
articleno = {1},
numpages = {51},
keywords = {sparse linear algebra, matrix multiply, GPU, Graph algorithm}
}

@article{stanimirovic2009performance,
  title={Performance comparison of storage formats for sparse matrices},
  author={Stanimirovic, Ivan P and Tasic, Milan B},
  journal={Ser. Mathematics and Informatics},
  volume={24},
  number={1},
  pages={39--51},
  year={2009}
}

@inproceedings{bulucc2009parallel,
  title={Parallel sparse matrix-vector and matrix-transpose-vector multiplication using compressed sparse blocks},
  author={Bulu{\c{c}}, Aydin and Fineman, Jeremy T and Frigo, Matteo and Gilbert, John R and Leiserson, Charles E},
  booktitle={Proceedings of the twenty-first annual symposium on Parallelism in algorithms and architectures},
  pages={233--244},
  year={2009}
}

@incollection{sarcar2022threading,
  title={Threading},
  author={Sarcar, Vaskaran},
  booktitle={Test Your Skills in C\# Programming: Review and Analyze Important Features of C\#},
  pages={385--429},
  year={2022},
  publisher={Springer}
}

@Inbook{Lee2020,
author="Lee, Joshua",
editor="Schintler, Laurie A.
and McNeely, Connie L.",
title="Multi-threading",
bookTitle="Encyclopedia of Big Data",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="1--4",
isbn="978-3-319-32001-4",
doi="10.1007/978-3-319-32001-4_404-1",
url="https://doi.org/10.1007/978-3-319-32001-4_404-1"
}
